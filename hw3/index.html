<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
    background-color: #e0c6cd;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>
<hr>
<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Zackary Oon</h2>
<hr>
<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-Zackoon/">Here</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/p4bunnyM5A.png" width="480px" />
          <figcaption align="middle">Mixed feelings about this bunny after this project...</figcaption>
      </tr>
  </table>
</div>

<div>

<h2 align="middle">Overview</h2>
<p>
  In Project 3, the main focus is on implementing various components of a ray tracer, including ray generation, primitive intersection testing, bounding volume hierarchy (BVH) construction, direct and indirect lighting, and adaptive sampling. We begin with generating rays from the camera and the implementing the Möller-Trumbore algorithm for triangle-ray intersection testing. We then construct a BVH to accelerate ray tracing, where splitting points are chosen based on a heuristic. 
  <br><br>
  We then add in acceleration structures such as the BVH. Next, direct lighting is implemented using uniform hemisphere sampling and importance sampling, with images rendered to compare the results of both methods. We move on to incorporate global illumination (indirect + direct lighting). Multi-bounce rendering and Russian Roulette rendering are explored to understand their impact on image quality and rendering efficiency. Finally, the concept of adaptive sampling is introduced to optimize rendering efficiency, with sample rate images and noise-free rendered results provided for analysis. Overall, in this project, we implement various ray tracing techniques to achieve appealing renders with the Cornell box and objects within it!
</p>
<br>
<hr>
<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>

<p>
  To generate a ray from a camera, we must “transform the image coordinates to camera space, generate the ray in the camera space, and finally transform it into a ray in the world space,” as given by the project spec. 
  <br><br>
  To clear any notation, note that the camera looks down along the -Z axis and this intersects with the image at (0, 0). The image has its bottom left corner and its top right corner as given in the image below. hFov and vFov are the horizontal and vertical (x and y) field of view angles.
</p>

<img src="images/Image Space.png" width=100%>
<figcaption align="center">Courtesy of the <a href="https://cs184.eecs.berkeley.edu/sp24/docs/hw3-1-part-1">CS184 Website</a></figcaption>

<hr>
<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>The steps I take in my implementation are:</p>
<ul>
  <li>normalize the image coordinates</li>
  <li>translate it by 0.5 in both the down and left directions (to center its center along the -Z axis)</li>
  <li>scale it by a factor of \(tan(\frac{radians(hFov)}{2}) * 2\) in the x-direction and \(tan(\frac{radians(vFov)}{2}) * 2\) in the y-direction</li>
  <li>we then know where the ray should be cast, in the camera’s coordinates</li>
  <li>cast a ray in this direction (still in camera coordinates)</li>
  <li>transform it to world coordinates</li>
  <li>set its min_t and max_t attributes to the near clipping plane and far clipping plane respectively</li>
  <li>and finally return the result!</li>
</ul>
<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  To calculate the intersection point (as well as if there is any at all), between a ray and a given triangle, we use the Moller-Trumbore intersection algorithm. 
  <br><br>
  The main idea is to set the ray’s equation equal to the triangle equation, which can be represented in the form of barycentric coordinates. We can then solve for the unknown parameters using vector operations (the ray’s t scaling, as well as the barycentric coordinate coefficients) — the explicit details are given in my code; however this is well explained by this image:
</p>
<img src="images/Cost = (1 div, 27 mul, 17 add).png" width=100%>
<figcaption align="center">Courtesy of Professors Ng (and O'Brien via the slide) <a href="https://cs184.eecs.berkeley.edu/sp24/docs/hw3-1-part-1">CS184 Website</a></figcaption>

<p>Now that we have recovered the parameters of the ray as well as where on the triangle we have intersected (via barycentric coordinates), we will now use barycentric interpolation to interpolate the normal at the intersection point (using the 3 vertex normals, n1 through n3).</p>


<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p1banana.png" align="middle" width="400px"/>
        <figcaption>A banana</figcaption>
      </td>
      <td>
        <img src="images/p1cow.png" align="middle" width="400px"/>
        <figcaption>A cow</figcaption>
      </td>
    </tr>
    <tr align="center">
        <img src="images/p1teapot.png" align="middle" width="400px"/>
        <figcaption>A teapot!</figcaption>
    </tr>
  </table>
</div>
<br>

<hr>
<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>

<hr>
<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  To construct my BVH, the steps I took were to: create a bounding box for that node (encapsulates all the primitives/nodes in it), check to see if the number of primitives in the node is less than the max allowed (base case), and create left and right vectors of primitives to recurse with. 
  <br> <br>
  Elaborating on how I created left and right nodes for the current node, I first counted the number of primitives (for each axis: x, y, z) that were < than the centroid (x, y, or z value depending on which axis I’m on) and the number >= to the centroid. I then took the absolute difference between those numbers, for each axis. The axis with the smallest absolute difference was the “best” choice, as it meant a more balanced BVH (while also maintaining some spatial locality, as the groups are split by their position relative to the centroid). 
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p2dragon.png" align="middle" width="400px"/>
        <figcaption>A dragon, 3.62s</figcaption>
      </td>
      <td>
        <img src="images/p2blob.png" align="middle" width="400px"/>
        <figcaption>A blob, 26.47s</figcaption>
      </td>
    </tr>
    <tr align="center">
        <img src="images/p2lucy.png" align="middle" width="400px"/>
        <figcaption>Lucy!, 4.33s</figcaption>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    In the table below, we compare the times it takes for geometries with and without BVH acceleration. It the numbers are below, as well as the images -- it seems like it sped up by some log scaling (as expected per the BVH).
</p>
<br>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p2bunny.png" align="middle" width="400px"/>
        <figcaption>A bunny, 79.06s vs. 1.62s</figcaption>
      </td>
      <td>
        <img src="images/p2building.png" align="middle" width="400px"/>
        <figcaption>A building, 96.12s vs 0.18s</figcaption>
      </td>
    </tr>
    <tr align="center">
        <img src="images/p2beetle.png" align="middle" width="400px"/>
        <figcaption>Beetle! 15.79s vs. 0.22s</figcaption>
    </tr>
  </table>
</div>

<hr>
<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->
<hr>
<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  To note, because we are using a diffuse material, we know that its BSDF will reflect incoming light equally in all directions on the hemisphere. Additionally, we work with emissive material (lights), and so to when implementing the zero bounce radiance, these are the only portions of the scene that get illuminated.
<br><br>
Now, when implementing direct lighting (which consists of the zero bounce radiance discussed above plus a one bounce radiance), there are two ways we approach it: via uniform hemisphere sampling and via importance sampling. The following explanations are concerned with the one bounce radiance (zero bounce is the same between the two, as described in the above paragraph). 
<br><br>
For direct hemisphere sampling we sample directions uniformly across the hemisphere from where the point is. For a given direction, we cast a ray from the initial point, towards the direction, and if there’s an intersection recorded, we will add that object’s contribution (only contributes if it has a non-zero emission) to the running total of the lighting at that initial point (if no intersection, then just continue). Note that the contribution is scaled by the appropriate material bsdf->f and the lambert term (where theta is the angle between the vector wi, which goes from the point to the light, and the intersection’s surface normal). More concretely, as given in the spec, we use this formula:
</p>

<img src="images/refmc.png" width=100%>
<figcaption align="center">Courtesy of the <a href="https://cs184.eecs.berkeley.edu/sp24/docs/hw3-1-part-1">CS184 Website</a></figcaption>

<p>
  In the case of direct hemisphere sampling p(wj) is just a constant factor of 2PI (as this is the sample domain size and we are working with a uniform density). 
  <br><br>
For importance sampling we sample from directions from which there are light sources (sample the directions starting from our point). In contrast to the direct hemisphere sampling, where we sample in uniformly random directions along the hemisphere, with importance sampling, we iterate through the available lights and only test directions that would “hit” these lights (assuming there’s no object in between). Contributions per light are normalized by the amount of samples per light (1 sample for point lights, and a chosen parameter for the number of samples per area light). These contributions are calculated in a similar fashion as in direct hemisphere sampling (barring the change what we normalize with). The ray’s max attribute is set to a magnitude just smaller than the distance from the point to the light source, and the idea is that if there’s any intersection recorded, there exists an object (assumed to be opaque) between the point and light source, meaning there’s no illumination from that direction.
<br><br>
Just as a note for both implementations, to ensure that the casted ray doesn’t hit the object intersection point it’s being casted, from we set the ray’s min attribute to be a small constant (rather than zero).
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/p3t3BunnyLow.png" align="middle" width="400px"/>
        <figcaption>Bunny, low samples, uniform</figcaption>
      </td>
      <td>
        <img src="images/p3t3BunnyHigh.png" align="middle" width="400px"/>
        <figcaption>Bunny, high samples, uniform</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/P3T4BunnyLow.png" align="middle" width="400px"/>
        <figcaption>Bunny, low samples, importance</figcaption>
      </td>
      <td>
        <img src="images/P3T4BunnyHigh.png" align="middle" width="400px"/>
        <figcaption>Bunny, high samples, importance</figcaption>
      </td>
    </tr>
    <tr>
      <img src="images/P3T4Dragon.png" width = "400px">
<figcaption align="center">Dragon, importance sampling!</figcaption>

    </tr>
    <br>
  </table>
</div>


<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/P3T4Sphere1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (spheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/P3T4Sphere4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/P3T4Sphere16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/P3T4Sphere64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    In the above images, we see how an increase in light rays does help with the shadows looking more natural!
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  We observe how with uniform hemisphere sampling, it is prone to a lot more noise, even with higher samples. With intuition that we only care (for direct illumination at least) about directions only towards light, we save a lot of noise and recover a cleaner image. The differences are best seen on the shadows.
</p>
<br>

<hr>
<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->
<hr>

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  The idea behind indirect lighting is to take into account contributions of light at a certain point from other objects (reflecting light), rather than just light sources. Thus, to implement indirect lighting, I recursively called one_bounce_radiance, as I continued to trace backwards through the wi vectors (incoming light vector). It is important to note that each intersection is scaled by the appropriate lambert term as well as by the pdf (and also the cpdf in the case of Russian Roulette). At each iteration, to know where to go next,  you sample according a cosine-weighted hemisphere distribution, record an intersection with the BVH at that direction from your current point, and continue. If no intersection is found, you just return the radiance at the current iteration (and pop back up).
  <br><br>
  The recursive calls go until the maximum ray depth was hit, or if the Russian Roulette flag was on, then it would terminate probabilistically (0.35 chance for termination), and any remaining rays would terminate at the max depth. To keep track of the recursion, we use the ray’s depth attribute, which is initially set to the max depth when the camera initially casts its viewing ray (when ray_trace_pixel calls the camera generate_ray function). 
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4GlobalSphere.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM5A.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4SphereDirect.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4SphereIndirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  It is important to note how direct lighting in the left image consists of 0 bounce and 1 bounce illumination, while the subsequent bounces are shown in the right side image (indirect illumination).
</p>

<h3>
  For CBbunny.dae, render the mth bounce views of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4bunnyM0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnyM2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnyM4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM5.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<br>

<h3>
  For CBbunny.dae, compare rendered views with accumulation on, with max_ray_depth set to 0, 1, 2, 3, and 5 (the -m flag). Use 1024 samples per pixel.
</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4BunnyM0A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4BunnyM1A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4BunnyM2A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM3A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4BunnyM4A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyM5A.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  For CBbunny.dae, compare rendered views with accumulation on, while using Russian Roulette, with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4bunnyrrM0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyrrM1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnyrrM2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyrrM3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnyrrM4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4bunnyrrM100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4sphere1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4sphere2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4sphere4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4sphere8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4sphere16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4sphere64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4sphere1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we may notice in the above gallery of images, as we increase the light bounces, we obtain a more robust image — there's some nice color bleeding into the surface shadows. Another effect we may notice in the most recent images above is how initially prone to noise this method is. This effect is greatly lessened as we increase the number of samples.
</p>
<br>

<hr>
<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->
<hr>

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  The idea behind adaptive sampling is to make Monte Carlo path tracing more efficient by decreasing the number of samples per pixel, on a per pixel basis. It does this by checking whether a pixel has a variance within an acceptable range, and if so, it’ll terminate sampling there. 
  <br><br>
  More specifically, we want \(I <= maxTolerance * \mu\), where \(I = 1.96 * \frac{SD}{sqrt(n)}\). Where \(\mu\) is the current mean illuminance and SD is the standard deviation of the illuminances for the samples that have taken place so far. 
  As long as we keep running track of \(s1 = \sum_{k=1}^{n}x_k\) and \(s2 = \sum_{k=1}^{n}x_k^2\) We can recover these terms via: \(\mu = \frac{s_1}{n}\) and \(SD =\frac{1}{n-1} * (s2 - \frac{s1^2}{n})\).
</p>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p5bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p5bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p5bench.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bench.dae)</figcaption>
      </td>
      <td>
        <img src="images/p5bench_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bench.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
